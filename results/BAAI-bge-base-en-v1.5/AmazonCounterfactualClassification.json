{
  "dataset_revision": "589b8cdf671a2e80745f3443de8e52f70d8c296f",
  "mteb_dataset_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.2.1.dev0",
  "test": {
    "EN": {
      "accuracy": 0.6576119402985074,
      "accuracy_stderr": 0.04079953873915354,
      "ap": 0.2858501984179188,
      "ap_stderr": 0.014260728986737518,
      "f1": 0.5969814184345468,
      "f1_stderr": 0.02729093353165169,
      "main_score": 0.6576119402985074
    },
    "EN-ext": {
      "accuracy": 0.6326836581709145,
      "accuracy_stderr": 0.0426246136416212,
      "ap": 0.14566680024635797,
      "ap_stderr": 0.013196635246186944,
      "f1": 0.5100815585114359,
      "f1_stderr": 0.027975150123452037,
      "main_score": 0.6326836581709145
    },
    "evaluation_time": 95.03
  },
  "validation": {
    "EN": {
      "accuracy": 0.6411940298507461,
      "accuracy_stderr": 0.04951626099043335,
      "ap": 0.23381654443066027,
      "ap_stderr": 0.01314091613019975,
      "f1": 0.5606013910238014,
      "f1_stderr": 0.027918411026525005,
      "main_score": 0.6411940298507461
    },
    "EN-ext": {
      "accuracy": 0.617867867867868,
      "accuracy_stderr": 0.048044528775019565,
      "ap": 0.13486663261219248,
      "ap_stderr": 0.011501513264145113,
      "f1": 0.4951504055364332,
      "f1_stderr": 0.02893753149745687,
      "main_score": 0.617867867867868
    },
    "evaluation_time": 64.33
  }
}